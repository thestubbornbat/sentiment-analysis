# -*- coding: utf-8 -*-
"""Black coffer text analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eBi0woOM8MuA-gHT6-fzRE2icRZK3D9k

# Importing necessary libraries
"""

!pip install BeautifulSoup4

import requests
from bs4 import BeautifulSoup
import pandas as pd
import xlrd

"""# url definition"""

url = "https://insights.blackcoffer.com/how-is-login-logout-time-tracking-for-employees-in-office-done-by-ai/"

url_file = xlrd.open_workbook('Input.xlsx')
url_list = []

"""Making a url list so as to make life easier"""

sheet = url_file.sheet_by_index(0)
sheet.cell_value(0, 0)
 
for i in range(sheet.nrows):
    url_entry = (sheet.cell_value(i, 1))
    url_list.append(url_entry)

print(url_list)
url_list.remove('URL')

"""# Getting article title and content"""

headers = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36',}
page_article_list = []
page_title_list = []
page_subheadings = []
for n in range(0,len(url_list)):
  r1 = requests.get(url_list[n],headers = headers)
  page =  r1.content
  soup1 = BeautifulSoup(page, 'html5lib')
  page_article = soup1.find_all('p', )
  page_title = soup1.find('title')
  title = page_title.get_text() + '.'
  title1 = title.replace('- Blackcoffer Insights','')
  page_title_list.append(page_title)
  list_paragraphs = []
  for p in range(0,len(page_article)):
    paragraph = page_article[p].get_text()
    list_paragraphs.append(paragraph)

  final_article1 = " ".join(list_paragraphs)
  final_article2 = " ".join((title1, final_article1))
  page_article_list.append(final_article2)

"""# Removing stop words"""

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

import nltk
nltk.download('punkt')
nltk.download('stopwords')

test = word_tokenize(page_article_list[0])

def remove_punc(string):
    punc = '''!()-[]{};:"\,'<>./?@#$%^&*_~'''
    for ele in string:  
        if ele in punc:  
            string = string.replace(ele, "") 
    return string

sentencelist = []
pronouns = ['I', 'We', 'My', 'Our', 'Ours', 'Us', 'us', 'our', 'ours', 'my', 'we']
tokenized_articles = []
pronoun_count_list = []
letter_count_list = []
word_count_all_list = []
for x in range(0, len(page_article_list)):
  pronoun_count = 0
  letter_count = 0
  word_count_all = 0
  tokenized = word_tokenize(page_article_list[x])
  c = tokenized.count('.')
  sentencelist.append(c)
  lis = [remove_punc(i) for i in tokenized]
  lis2 = [y for y in lis if y]
  for i in lis2:
    letter_count = letter_count + len(i)
  letter_count_list.append(letter_count)
  word_count_all = len(lis2)
  for i in range(len(lis2)):
    for j in range(len(pronouns)):
      if lis2[i] == pronouns[j]:
        pronoun_count = pronoun_count+1
  pronoun_count_list.append(pronoun_count)
  word_count_all_list.append(word_count_all)
  tokenized_articles.append(lis2)

Processed = []
for x in range(0,len(tokenized_articles)):
  Stopped = [word for word in tokenized_articles[x] if not word in stopwords.words()]
  Processed.append(Stopped)

df = pd.read_excel("dictionary1.xlsx")

pwords = df.loc[df['Positive'] != 0, 'Word'] 
pwords_list = pwords.tolist()

nwords = df.loc[df['Negative'] != 0, 'Word'] 
nwords_list = nwords.tolist()

def remove_items(list, item):
      
    # using list comprehension to perform the task
    res = [i for i in list if i != item]
  
    return res

Processed_final = []
complex_count_list = []
syllable_count_list = []
for x in range(0, len(Processed)):
  Uppercased = [m.upper() for m in Processed[x]]
  Uppercased2 = remove_items(Uppercased,'â€™')
  c_count = 0
  s_count = 0
  for i in Uppercased2:
    syllable_count=0
    complex_count=0
    for w in i:
      if(w=='A' or w=='E' or w=='I' or w=='O' or w=='U'):
        syllable_count=syllable_count+1
    if syllable_count > 2:
      complex_count = complex_count+1
    if complex_count == 1:
      c_count = c_count+1    
    if (syllable_count == 0 or syllable_count != 0):
      s_count = s_count + syllable_count
  complex_count_list.append(c_count)
  syllable_count_list.append(s_count)
  Processed_final.append(Uppercased2)

word_count = []
for x in range(0,len(Processed_final)):
  cnt = len(Processed_final[x])
  word_count.append(cnt)

counterpos_list=[]

for x in range(0,len(Processed_final)):
  counterpos= 0
  for y in Processed_final[x]:
    for z in pwords_list:
      if y == z:
        counterpos += 1
  counterpos_list.append(counterpos)

print(counterpos_list)

counterneg_list=[]
for x in range(0,len(Processed_final)):
  counterneg= 0
  for y in Processed_final[x]:
    for z in nwords_list:
      if y == z:
        counterneg -= 1
  counterneg_list.append(counterneg)

print(counterneg_list)

df = pd.read_excel('Output.xlsx')

df['POSITIVE SCORE'] = counterpos_list

df['NEGATIVE SCORE'] = counterneg_list

"""defining all functions"""

def polarity_score(a,b):
  return (a-b)/((a+b) + 0.000001)

def subjectivity_score(a,b,c):
  return (a+b)/(c+0.000001)

def average_sentence_length(a,b):
  return (a/b)

def complex_word_percentage(a,b):
  return ((a/b)*100)

def fog_index(a,b):
  return (0.4*(a+b))  

def syllable_count_per_word(a,b):
  return (a/b)  

def avg_word_length(a,b):
  return (a/b)

avg_word_length_list = []
for i in range(len(word_count)):
  avg_word_length_list.append(avg_word_length(letter_count_list[i], word_count_all_list[i]))

syllable_count_pw_list = []
for i in range(len(word_count)):
  syllable_count_pw_list.append(syllable_count_per_word(syllable_count_list[i], word_count[i]))

comp_percent_list = []
for i in range(len(complex_count_list)):
  comp_percent_list.append(complex_word_percentage(complex_count_list[i],word_count[i]))

fog_index_list = []
for i in range(len(comp_percent_list)):
  fog_index_list.append(fog_index(avg_sentence_len_list[i],comp_percent_list[i]))

df['FOG INDEX'] = fog_index_list

print(comp_percent_list)

counterneg_list2 = [-1*x for x in counterneg_list]

pol_list = []
for i in range(len(counterpos_list)):
  pol_list.append(polarity_score(counterpos_list[i],counterneg_list2[i]))

print(pol_list)

df['POLARITY SCORE'] = pol_list

sub_list = []
for i in range(len(counterpos_list)):
  sub_list.append(subjectivity_score(counterpos_list[i],counterneg_list2[i],word_count[i]))

print(sub_list)

df['SUBJECTIVITY SCORE'] = sub_list

df['COMPLEX WORD COUNT'] = complex_count_list

df['AVG NUMBER OF WORDS PER SENTENCE'] = avg_sentence_len_list

df['WORD COUNT'] = word_count

df['PERSONAL PRONOUNS'] = pronoun_count_list

avg_sentence_len_list = []
for i in range(0,len(word_count)):
  avg_sentence_len_list.append(average_sentence_length(word_count[i], sentencelist[i]))

print(avg_sentence_len_list)

df['AVG SENTENCE LENGTH'] = avg_sentence_len_list

df['PERCENTAGE OF COMPLEX WORDS'] = comp_percent_list

df['SYLLABLE PER WORD'] = syllable_count_pw_list

df['AVG WORD LENGTH'] = avg_word_length_list

df

df.to_excel('Output1.xlsx')